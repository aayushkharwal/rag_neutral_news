{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "BOIu8RpHQ-sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "BASE_URL = \"https://www.theflipside.io\"\n",
        "ARCHIVE_URL = f\"{BASE_URL}/archives\"\n",
        "SEARCH_URL = f\"{BASE_URL}/search\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "\n",
        "def search_fallback_link_for_date(date_obj):\n",
        "    search_query = f\"{date_obj.strftime('%b')} {date_obj.day}, {date_obj.year}\"\n",
        "    encoded_query = quote(search_query)\n",
        "    search_url = f\"{SEARCH_URL}?query={encoded_query}\"\n",
        "\n",
        "    print(f\"Searching fallback for: {search_query} - {search_url}\")\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(search_url, headers=HEADERS)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Failed to fetch search page: {resp.status_code}\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "        result = soup.select_one(\"div.search-result-item a.link\")\n",
        "\n",
        "        if result:\n",
        "            text = result.text.strip()\n",
        "            if \", \" in text:\n",
        "                try:\n",
        "                    title, date_str = text.rsplit(\" , \", 1)\n",
        "                    result_date = datetime.strptime(date_str, \"%b %d, %Y\")\n",
        "                    if result_date.date() == date_obj.date():\n",
        "                        url = result['href']\n",
        "                        print(f\"Search matched: {title.strip()} - {url}\")\n",
        "                        return {\n",
        "                            \"date\": date_obj.strftime(\"%Y-%m-%d\"),\n",
        "                            \"title\": title.strip(),\n",
        "                            \"url\": url if url.startswith(\"http\") else BASE_URL + url,\n",
        "                            \"found_by\": \"search\"\n",
        "                        }\n",
        "                    else:\n",
        "                        print(f\"First result does not match target date ({result_date.date()} != {date_obj.date()})\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to parse result: {text}\")\n",
        "        else:\n",
        "            print(\"No search results found.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception during search: {e}\")\n",
        "\n",
        "    print(\"No fallback result for this date.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def fetch_flipside_articles(start_date_str, end_date_str, output_filename=\"flipside_links_full.json\"):\n",
        "    output_path = os.path.join(\"/content/drive/My Drive\", output_filename)\n",
        "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
        "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
        "\n",
        "    print(\"Fetching Flipside archives...\")\n",
        "    resp = requests.get(ARCHIVE_URL, headers=HEADERS)\n",
        "    resp.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    archive_entries = soup.find_all('a', class_='link-block-3 w-inline-block')\n",
        "\n",
        "\n",
        "    archive_map = {}\n",
        "    for entry in archive_entries:\n",
        "        href = entry.get('href')\n",
        "        date_parts = entry.find_all('h4', class_='heading-12')\n",
        "        title_tag = entry.find('h3', class_='heading-13')\n",
        "\n",
        "        if len(date_parts) == 2:\n",
        "            try:\n",
        "                date = datetime.strptime(f\"{date_parts[0].text.strip()} {date_parts[1].text.strip()} 2025\", \"%B %d %Y\")\n",
        "                full_url = href if href.startswith(\"http\") else BASE_URL + href\n",
        "                archive_map[date.date()] = {\n",
        "                    \"date\": date.strftime(\"%Y-%m-%d\"),\n",
        "                    \"title\": title_tag.text.strip() if title_tag else \"\",\n",
        "                    \"url\": full_url,\n",
        "                    \"found_by\": \"archive\"\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing archive entry: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "    results = []\n",
        "    current = start_date\n",
        "    id_counter = 1\n",
        "\n",
        "    print(\"\\n Checking articles between:\", start_date_str, \"to\", end_date_str)\n",
        "    while current <= end_date:\n",
        "        print(f\"\\n Processing date: {current.strftime('%Y-%m-%d')}\")\n",
        "        entry = archive_map.get(current.date())\n",
        "\n",
        "        if not entry:\n",
        "            entry = search_fallback_link_for_date(current)\n",
        "\n",
        "        if entry:\n",
        "            entry[\"id\"] = id_counter\n",
        "            results.append(entry)\n",
        "            print(f\"Added [{entry['found_by']}]: {entry['url']}\")\n",
        "            id_counter += 1\n",
        "        else:\n",
        "            print(\"No article found for this date.\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n Done! Saved {len(results)} entries to: {output_path}\")\n",
        "\n",
        "fetch_flipside_articles(\"2019-01-01\", \"2020-12-31\", \"flipside_links_19-20.json\")\n"
      ],
      "metadata": {
        "id": "bGWI9dlCXjBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "\n",
        "input_json_path = \"/content/drive/MyDrive/flipside_links_19-20.json\"\n",
        "output_json_path = \"/content/drive/My Drive/flipside_articles_19-20.json\"\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "\n",
        "\n",
        "def extract_articles_from_section(blocks, side, headline):\n",
        "    filtered_blocks = []\n",
        "    for block in blocks:\n",
        "        class_list = block.get(\"class\", [])\n",
        "        if \"w-dyn-bind-empty\" not in class_list:\n",
        "            filtered_blocks.append(block)\n",
        "\n",
        "    print(f\"Found {len(filtered_blocks)} valid {side} blocks\")\n",
        "\n",
        "    articles = []\n",
        "    if len(filtered_blocks) == 1:\n",
        "        print(f\"Single block in {side}, attempting to split by </strong>...\")\n",
        "        raw_html = str(filtered_blocks[0])\n",
        "        segments = raw_html.split(\"</strong>\")\n",
        "        for idx, segment in enumerate(segments):\n",
        "            seg_soup = BeautifulSoup(segment, \"html.parser\")\n",
        "            text = seg_soup.get_text().strip()\n",
        "            link_tag = seg_soup.find(\"a\")\n",
        "            if text and len(text)>10:\n",
        "                print(f'##{text}##')\n",
        "                articles.append({\n",
        "                    \"headline\": headline,\n",
        "                    \"text\": text,\n",
        "                    \"link\": link_tag[\"href\"] if link_tag else \"\"\n",
        "                })\n",
        "                print(f\"{side.capitalize()}[{idx}] - {'link' if link_tag else 'no link'}\")\n",
        "        return articles\n",
        "\n",
        "    for idx, block in enumerate(filtered_blocks):\n",
        "        soup = BeautifulSoup(str(block), \"html.parser\")\n",
        "        text = soup.get_text().strip()\n",
        "        link_tag = soup.find(\"a\")\n",
        "        if text and len(text)>10:\n",
        "            articles.append({\n",
        "                \"headline\": headline,\n",
        "                \"text\": text,\n",
        "                \"link\": link_tag[\"href\"] if link_tag else \"\"\n",
        "            })\n",
        "            print(f\"{side.capitalize()}[{idx}] - {'link' if link_tag else 'no link'}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parse_article_page(entry):\n",
        "    url = entry[\"url\"]\n",
        "    print(f\"\\n Fetching: {url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.encoding = 'utf-8'\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to fetch article\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        parsed_entry = {\n",
        "            \"id\": entry[\"id\"],\n",
        "            \"date\": entry[\"date\"],\n",
        "            \"source_url\": entry[\"url\"],\n",
        "            \"source_title\": entry[\"title\"],\n",
        "            \"found_by\": entry[\"found_by\"],\n",
        "            \"neutral\": [],\n",
        "            \"left\": [],\n",
        "            \"right\": []\n",
        "        }\n",
        "\n",
        "        print(\"Parsing neutral section...\")\n",
        "        neutral_block = soup.find(\"div\", class_=\"rich-text-block-2\")\n",
        "        if neutral_block:\n",
        "            raw_html = str(neutral_block)\n",
        "            sections = raw_html.split(\"</strong>\")\n",
        "            for idx, section in enumerate(sections):\n",
        "                sec_soup = BeautifulSoup(section, \"html.parser\")\n",
        "                link_tag = sec_soup.find(\"a\")\n",
        "                text = sec_soup.get_text().strip()\n",
        "                if link_tag and text:\n",
        "                    parsed_entry[\"neutral\"].append({\n",
        "                        \"headline\": entry[\"title\"],\n",
        "                        \"text\": text,\n",
        "                        \"link\": link_tag[\"href\"]\n",
        "                    })\n",
        "                    print(f\"Neutral[{idx}] - {link_tag['href'][:50]}...\")\n",
        "\n",
        "\n",
        "\n",
        "        def get_valid_heading(soup, class_name):\n",
        "          candidates = soup.find_all(\"h1\", class_=class_name)\n",
        "          for tag in candidates:\n",
        "              if tag.get_text(strip=True) and \"w-dyn-bind-empty\" not in tag.get(\"class\", []):\n",
        "                  return tag\n",
        "          return None\n",
        "\n",
        "\n",
        "        left_column_heading = get_valid_heading(soup, \"from-the-left-heading\")\n",
        "        right_column_heading = get_valid_heading(soup, \"from-the-right-heading\")\n",
        "\n",
        "        left_label = left_column_heading.get_text(strip=True).split()[-1].lower() if left_column_heading else \"\"\n",
        "        right_label = right_column_heading.get_text(strip=True).split()[-1].lower() if right_column_heading else \"\"\n",
        "\n",
        "        is_flipped = left_label == \"right\" or right_label == \"left\"\n",
        "        print(f\"Left heading: '{left_label}', Right heading: '{right_label}' - Flipped: {is_flipped}\")\n",
        "\n",
        "\n",
        "        left_intro = soup.select_one(\"div.paragraph-6.left.intro\")\n",
        "        left_headline = left_intro.get_text(strip=True) if left_intro else \"\"\n",
        "\n",
        "        right_intro = soup.select_one(\"div.paragraph-6.right.intro\")\n",
        "        right_headline = right_intro.get_text(strip=True) if right_intro else \"\"\n",
        "\n",
        "        left_blocks = soup.select(\"div.paragraph-6.left.bullet\")\n",
        "        right_blocks = soup.select(\"div.paragraph-6.right.bullet\")\n",
        "\n",
        "        if is_flipped:\n",
        "            print(\"Flipped detected. Swapping columns.\")\n",
        "            print(\"Parsing left section...\")\n",
        "            parsed_entry[\"left\"] = extract_articles_from_section(right_blocks, \"left\", left_headline)\n",
        "            print(\"Parsing right section...\")\n",
        "            parsed_entry[\"right\"] = extract_articles_from_section(left_blocks, \"right\", right_headline)\n",
        "        else:\n",
        "            print(\"Parsing left section...\")\n",
        "            parsed_entry[\"left\"] = extract_articles_from_section(left_blocks, \"left\", left_headline)\n",
        "            print(\"Parsing right section...\")\n",
        "            parsed_entry[\"right\"] = extract_articles_from_section(right_blocks, \"right\", right_headline)\n",
        "\n",
        "\n",
        "        return parsed_entry\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception while parsing: {e}\")\n",
        "        return None\n",
        "\n",
        "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    input_data = json.load(f)\n",
        "\n",
        "parsed_results = []\n",
        "\n",
        "for entry in input_data:\n",
        "    result = parse_article_page(entry)\n",
        "    if result:\n",
        "        parsed_results.append(result)\n",
        "\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(parsed_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n Done! Parsed {len(parsed_results)} articles and saved to: {output_json_path}\")\n"
      ],
      "metadata": {
        "id": "EiXATvLSMaVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "def fetch_article_data(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Skipped - {url} (HTTP {response.status_code})\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"\"\n",
        "\n",
        "        paragraphs = soup.find_all('p')\n",
        "        body = \"\\n\".join(p.get_text().strip() for p in paragraphs if p.get_text(strip=True))\n",
        "\n",
        "        if len(body) < 300:\n",
        "            print(f\"Skipped - {url} (Too short: {len(body)} chars)\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"headline\": title,\n",
        "            \"text\": body\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception fetching {url} - {str(e).splitlines()[0]}\")\n",
        "        return None\n",
        "\n",
        "def process_entry(entry):\n",
        "    print(f\"\\nProcessing ID {entry['id']} â€” {entry['source_title']}\")\n",
        "    output_entry = {\n",
        "        \"id\": entry[\"id\"],\n",
        "        \"date\": entry[\"date\"],\n",
        "        \"source_url\": entry[\"source_url\"],\n",
        "        \"source_title\": entry[\"source_title\"],\n",
        "        \"found_by\": entry[\"found_by\"],\n",
        "        \"neutral\": [],\n",
        "        \"left\": [],\n",
        "        \"right\": []\n",
        "    }\n",
        "\n",
        "    for side in [\"neutral\", \"left\", \"right\"]:\n",
        "        print(f\"Fetching original {side} articles...\")\n",
        "        for art in entry.get(side, []):\n",
        "            url = art.get(\"link\")\n",
        "            if not url:\n",
        "                print(f\"Skipped - missing link in {side} article\")\n",
        "                continue\n",
        "\n",
        "            fetched = fetch_article_data(url)\n",
        "            if fetched:\n",
        "                output_entry[side].append({\n",
        "                    \"headline\": fetched[\"headline\"],\n",
        "                    \"text\": fetched[\"text\"],\n",
        "                    \"link\": url\n",
        "                })\n",
        "\n",
        "    return output_entry\n",
        "\n",
        "INPUT_PATH = \"/content/drive/MyDrive/flipside_articles_20-21.json\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/flipside_full_articles_20-21.json\"\n",
        "\n",
        "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "final_output = []\n",
        "\n",
        "for idx, entry in enumerate(data):\n",
        "    parsed = process_entry(entry)\n",
        "\n",
        "    if any(parsed[section] for section in [\"neutral\", \"left\", \"right\"]):\n",
        "        final_output.append(parsed)\n",
        "\n",
        "        if idx % 5 == 0:\n",
        "            with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"Backup saved at {OUTPUT_PATH} after {idx+1} entries\")\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n Done! Saved {len(final_output)} filtered entries to: {OUTPUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "ADm0jBz482mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper4k lxml_html_clean\n",
        "!python -m nltk.downloader punkt\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "from newspaper import Article\n",
        "\n",
        "INPUT_PATH = \"/content/drive/MyDrive/flipside_articles_21-22.json\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/flipside_4k_articles_21-22.json\"\n",
        "\n",
        "\n",
        "def fetch_article_data(url):\n",
        "    try:\n",
        "        article = Article(url, language='en')\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        title = article.title.strip() if article.title else \"\"\n",
        "        text = article.text.strip()\n",
        "\n",
        "        if len(text) < 300:\n",
        "            print(f\"Skipping {url} (Too short: {len(text)} chars)\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"headline\": title,\n",
        "            \"text\": text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url} - {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_entry(entry):\n",
        "    print(f\"\\n Processing ID {entry['id']} â€” {entry['source_title']}\")\n",
        "    output_entry = {\n",
        "        \"id\": entry[\"id\"],\n",
        "        \"date\": entry[\"date\"],\n",
        "        \"source_url\": entry[\"source_url\"],\n",
        "        \"source_title\": entry[\"source_title\"],\n",
        "        \"found_by\": entry[\"found_by\"],\n",
        "        \"neutral\": [],\n",
        "        \"left\": [],\n",
        "        \"right\": []\n",
        "    }\n",
        "\n",
        "    for side in [\"neutral\", \"left\", \"right\"]:\n",
        "        print(f\"Fetching {side} articles...\")\n",
        "        for art in entry.get(side, []):\n",
        "            url = art.get(\"link\")\n",
        "            if not url:\n",
        "                print(f\"Missing link in {side}\")\n",
        "                continue\n",
        "\n",
        "            fetched = fetch_article_data(url)\n",
        "            if fetched:\n",
        "                output_entry[side].append({\n",
        "                    \"headline\": fetched[\"headline\"],\n",
        "                    \"text\": fetched[\"text\"],\n",
        "                    \"link\": url\n",
        "                })\n",
        "\n",
        "    return output_entry\n",
        "\n",
        "\n",
        "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "final_output = []\n",
        "\n",
        "for idx, entry in enumerate(data):\n",
        "    parsed = process_entry(entry)\n",
        "\n",
        "    if any(parsed[section] for section in [\"neutral\", \"left\", \"right\"]):\n",
        "        final_output.append(parsed)\n",
        "\n",
        "        if (idx + 1) % 5 == 0:\n",
        "            with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"Backup saved after {idx + 1} entries - {OUTPUT_PATH}\")\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n Done! Saved {len(final_output)} entries with valid articles to: {OUTPUT_PATH}\")\n"
      ],
      "metadata": {
        "id": "c9CCbCn8XcX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/flipside_4k_articles_23-25.json\"\n",
        "\n",
        "with open(OUTPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "complete_count = 0\n",
        "for entry in data:\n",
        "    if all(entry.get(section) for section in [\"neutral\", \"left\", \"right\"]):\n",
        "        complete_count += 1\n",
        "\n",
        "print(f\"Total entries with all 3 sides: {complete_count} out of {len(data)}\")\n"
      ],
      "metadata": {
        "id": "swPhHY17Mcyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGV_egxq72Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzRrFDJ71ahO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rS2S362P1aSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import torch\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load model/tokenizer\n",
        "model_name = \"allenai/led-base-16384\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"\\nâœ… Using device: {device}\\n\")\n",
        "\n",
        "# JSON file paths\n",
        "json_files = [\"/kaggle/input/flipside-articles/flipside_4K_articles_23-25.json\", \"/kaggle/input/flipside-articles/flipside_4k_articles_21-22.json\", \"/kaggle/input/flipside-articles/flipside_4k_articles_19-20.json\"]\n",
        "\n",
        "\n",
        "triplets = []\n",
        "input_lengths = []\n",
        "target_lengths = []\n",
        "total_lengths = []\n",
        "\n",
        "# Load and process triplets\n",
        "for json_file in json_files:\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "        for entry in data:\n",
        "            left_text = \" \".join([a['text'] for a in entry.get(\"left\", [])])\n",
        "            right_text = \" \".join([a['text'] for a in entry.get(\"right\", [])])\n",
        "            neutral_text = \" \".join([a['text'] for a in entry.get(\"neutral\", [])])\n",
        "\n",
        "            if left_text.strip() and right_text.strip() and neutral_text.strip():\n",
        "                input_text = f\"Left:\\n{left_text}\\n\\nRight:\\n{right_text}\"\n",
        "                input_tokens = tokenizer(input_text, truncation=True, max_length=16384)\n",
        "                target_tokens = tokenizer(neutral_text, truncation=True, max_length=16384)\n",
        "\n",
        "                input_len = len(input_tokens[\"input_ids\"])\n",
        "                target_len = len(target_tokens[\"input_ids\"])\n",
        "\n",
        "                input_lengths.append(input_len)\n",
        "                target_lengths.append(target_len)\n",
        "                total_lengths.append(input_len + target_len)\n",
        "\n",
        "                triplets.append((input_text, neutral_text))\n",
        "\n",
        "# Average lengths\n",
        "print(\"ðŸ“ Token Length Stats:\")\n",
        "print(f\"Input (Left+Right): {sum(input_lengths)/len(input_lengths):.2f} tokens (avg)\")\n",
        "print(f\"Target (Neutral):   {sum(target_lengths)/len(target_lengths):.2f} tokens (avg)\")\n",
        "print(f\"Total (Combined):   {sum(total_lengths)/len(total_lengths):.2f} tokens (avg)\")\n",
        "\n",
        "# ðŸ“Š Plot distributions\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(input_lengths, bins=30, color='dodgerblue', edgecolor='black')\n",
        "plt.title(\"Input Token Lengths (Left + Right)\")\n",
        "plt.xlabel(\"Tokens\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(target_lengths, bins=30, color='mediumseagreen', edgecolor='black')\n",
        "plt.title(\"Target Token Lengths (Neutral)\")\n",
        "plt.xlabel(\"Tokens\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(total_lengths, bins=30, color='salmon', edgecolor='black')\n",
        "plt.title(\"Total Token Lengths (Input + Target)\")\n",
        "plt.xlabel(\"Tokens\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "idx = 0\n",
        "input_text, target_text = triplets[idx]\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=16384)\n",
        "print(\"Token length:\", len(inputs[\"input_ids\"][0]))\n",
        "print(\"Sample input:\\n\", input_text[:500])  # Preview first 500 chars\n",
        "\n",
        "import time\n",
        "\n",
        "generated_summaries = []\n",
        "references = []\n",
        "\n",
        "max_gen_len = 1024         # Max length of generated summary\n",
        "max_input_len = 16000      # Slightly under LED limit to be safe\n",
        "skipped_count = 0\n",
        "\n",
        "print(\"âš™ï¸ Running inference on first 10 samples...\")\n",
        "\n",
        "for idx, (input_text, target_text) in enumerate(tqdm(triplets[:10])):\n",
        "    try:\n",
        "        # Tokenize input (left + right)\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_input_len\n",
        "        )\n",
        "\n",
        "        token_len = inputs[\"input_ids\"].shape[1]\n",
        "        print(f\"[{idx}] Input token length: {token_len}\")\n",
        "\n",
        "        # Skip empty or malformed\n",
        "        if token_len == 0:\n",
        "            print(f\"âš ï¸ Skipping empty input at index {idx}\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # Move to device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        global_attention_mask = torch.zeros_like(inputs[\"input_ids\"]).to(device)\n",
        "        global_attention_mask[:, 0] = 1  # Only first token has global attention\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                global_attention_mask=global_attention_mask,\n",
        "                max_length=max_gen_len,\n",
        "                num_beams=4,\n",
        "                no_repeat_ngram_size=3,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Decode and save\n",
        "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        generated_summaries.append(generated_text)\n",
        "        references.append(target_text)\n",
        "\n",
        "        # Clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        time.sleep(0.2)  # Optional cooldown\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ FAILED at index {idx}: {e}\")\n",
        "        skipped_count += 1\n",
        "        torch.cuda.empty_cache()\n",
        "        continue\n",
        "\n",
        "# Final status\n",
        "print(f\"\\nâœ… Inference done.\")\n",
        "print(f\"ðŸ”¢ Total attempted: {len(triplets[:100])}\")\n",
        "print(f\"âœ… Generated: {len(generated_summaries)}\")\n",
        "print(f\"âš ï¸ Skipped: {skipped_count}\")\n",
        "\n",
        "\n",
        "import nltk\n",
        "import evaluate\n",
        "\n",
        "# 1. Ensure metrics are installed\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "# 2. BERTScore and ROUGE expect plain strings\n",
        "assert all(isinstance(p, str) for p in generated_summaries)\n",
        "assert all(isinstance(r, str) for r in references)\n",
        "\n",
        "# âœ… Evaluate BERTScore\n",
        "bertscore_result = bertscore.compute(predictions=generated_summaries, references=references, lang=\"en\")\n",
        "\n",
        "# âœ… Evaluate ROUGE\n",
        "rouge_result = rouge.compute(predictions=generated_summaries, references=references)\n",
        "\n",
        "# 3. BLEU requires tokenized input (double tokenized for refs)\n",
        "bleu_preds = [nltk.word_tokenize(p) for p in generated_summaries]\n",
        "bleu_refs = [[nltk.word_tokenize(r)] for r in references]  # <- list of list of tokens\n",
        "\n",
        "# âœ… Evaluate BLEU\n",
        "bleu_result = bleu.compute(predictions=bleu_preds, references=bleu_refs)\n",
        "\n",
        "# 4. Print results\n",
        "print(\"\\nðŸ” Evaluation Metrics:\")\n",
        "print(f\"BERTScore - Precision: {sum(bertscore_result['precision']) / len(bertscore_result['precision']):.4f}\")\n",
        "print(f\"BERTScore - Recall:    {sum(bertscore_result['recall']) / len(bertscore_result['recall']):.4f}\")\n",
        "print(f\"BERTScore - F1:        {sum(bertscore_result['f1']) / len(bertscore_result['f1']):.4f}\")\n",
        "print(f\"ROUGE: {rouge_result}\")\n",
        "print(f\"BLEU:  {bleu_result['bleu']:.4f}\")\n"
      ],
      "metadata": {
        "id": "STomWabyOUXc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}