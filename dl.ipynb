{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc49e5-bd93-425a-bc47-179dc6a7ff26",
   "metadata": {
    "id": "2ddc49e5-bd93-425a-bc47-179dc6a7ff26",
    "outputId": "9889a3d7-dba9-4351-c318-21f7d97a81ea"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instructions to install packages and libraries:\n",
    "1. !nvidia-smi (Check CUDA)\n",
    "https://www.carc.usc.edu/user-guides/data-science/building-conda-environment\n",
    "2. module purge\n",
    "3. module load conda\n",
    "4. mamba init bash\n",
    "5. source ~/.bashrc\n",
    "6. mamba create --name <env_name>\n",
    "7. mamba activate <env_name>\n",
    "8. mamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "9. mamba install other libraries\n",
    "10. Run a batch job\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bPgZ_AIQFxya",
   "metadata": {
    "id": "bPgZ_AIQFxya"
   },
   "outputs": [],
   "source": [
    "# Batch Job Script. May have to change the env\n",
    "# #!/bin/bash\n",
    "\n",
    "# #SBATCH --account=yzhao010_1531\n",
    "# #SBATCH --partition=gpu\n",
    "# #SBATCH --nodes=1\n",
    "# #SBATCH --ntasks=1\n",
    "# #SBATCH --cpus-per-task=4\n",
    "# #SBATCH --gpus-per-task=a100:1\n",
    "# #SBATCH --mem=16G\n",
    "# #SBATCH --time=2:30:00\n",
    "# #SBATCH --output=/home1/kharwal/logs_stdout.txt\n",
    "# #SBATCH --error=/home1/kharwal/logs_stderr.txt\n",
    "\n",
    "# module purge\n",
    "# module load conda\n",
    "# eval \"$(conda shell.bash hook)\"\n",
    "\n",
    "# mamba activate /home1/kharwal/.conda/envs/dl_100\n",
    "\n",
    "# python dl_100.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a12e0d-8e05-43e2-9f3b-615ec599217c",
   "metadata": {
    "id": "19a12e0d-8e05-43e2-9f3b-615ec599217c",
    "outputId": "6f0e7179-181d-4945-8ddb-66955ecfe035"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Political Neutrality Model: Fine-tuning MPT-7B for neutral article generation\n",
    "-------------------\n",
    "This script fine-tunes a 7B parameter model to generate politically neutral\n",
    "content given left-leaning and right-leaning articles using QLoRA techniques.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff50da-5d48-4fb9-8e64-441cd4f6ebc2",
   "metadata": {
    "id": "d1ff50da-5d48-4fb9-8e64-441cd4f6ebc2"
   },
   "outputs": [],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"mosaicml/mpt-7b-8k-instruct\", \n",
    "    \"root_dir\": \"/home1/kharwal/\",              \n",
    "    \"output_dir\": \"mpt-lora-neutral\",  \n",
    "\n",
    "    \"max_seq_length\": 6000,                     \n",
    "    \"per_device_batch_size\": 1,                 \n",
    "    \"gradient_accumulation_steps\": 8,           \n",
    "    \"learning_rate\": 2e-4,                      \n",
    "    \"num_train_epochs\": 5,     \n",
    "\n",
    "    \"lora_r\": 8,                                \n",
    "    \"lora_alpha\": 16,                           \n",
    "    \"lora_dropout\": 0.05,                       \n",
    "    \"target_modules\": [\"Wqkv\", \"out_proj\"],     \n",
    "\n",
    "    \"weight_decay\": 0.01,                       \n",
    "    \"warmup_steps\": 25,                         \n",
    "\n",
    "    \"max_new_tokens\": 1800,                     \n",
    "    \"evaluate_every\": 30,                      \n",
    "    \"eval_accumulation_steps\": 8,               \n",
    "    \"save_total_limit\": 2,                      \n",
    "    \"save_strategy\": \"steps\",                   \n",
    "\n",
    "    \"json_files\": [                             \n",
    "        \"flipside_4K_articles_23-25.json\",\n",
    "        \"flipside_4k_articles_21-22.json\",\n",
    "        \"flipside_4k_articles_19-20.json\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f30d55-a720-4bec-b97a-a4e58a9b38f7",
   "metadata": {
    "id": "06f30d55-a720-4bec-b97a-a4e58a9b38f7"
   },
   "outputs": [],
   "source": [
    "def load_all_data(file_paths: List[str], root_dir: str) -> List[Dict[str, Any]]:\n",
    "    all_entries = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(root_dir, path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"Loading data from {full_path}\")\n",
    "            with open(full_path, \"r\") as f:\n",
    "                all_entries.extend(json.load(f))\n",
    "        else:\n",
    "            print(f\"[WARN] File not found: {full_path}\")\n",
    "\n",
    "    print(f\"Loaded {len(all_entries)} total entries from {len(file_paths)} files\")\n",
    "    return all_entries\n",
    "\n",
    "def generate_triplets(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    triplets = []\n",
    "\n",
    "    for entry in data:\n",
    "        left_articles = entry.get(\"left\", [])\n",
    "        right_articles = entry.get(\"right\", [])\n",
    "        neutral_articles = entry.get(\"neutral\", [])\n",
    "\n",
    "        if not (left_articles and right_articles and neutral_articles):\n",
    "            continue\n",
    "\n",
    "        left_cycle = cycle(left_articles)\n",
    "        right_cycle = cycle(right_articles)\n",
    "\n",
    "        for neutral in neutral_articles:\n",
    "            left = next(left_cycle)\n",
    "            right = next(right_cycle)\n",
    "\n",
    "            left_combined = f\"{left['headline']}. {left['text']}\"\n",
    "            right_combined = f\"{right['headline']}. {right['text']}\"\n",
    "            neutral_combined = f\"{neutral['headline']}. {neutral['text']}\"\n",
    "\n",
    "            triplets.append({\n",
    "                \"left\": left_combined,\n",
    "                \"right\": right_combined,\n",
    "                \"neutral\": neutral_combined,\n",
    "                \"meta\": {\n",
    "                    \"event_id\": entry[\"id\"],\n",
    "                    \"date\": entry[\"date\"],\n",
    "                    \"source_title\": entry[\"source_title\"]\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(f\"Generated {len(triplets)} triplets\")\n",
    "    return triplets\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length=4096):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        prompt = f\"<|left|>\\n{row['left']}\\n<|right|>\\n{row['right']}\\n<|neutral|>\\n{row['neutral']}\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone()\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "\n",
    "    tokenizer: AutoTokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids = [b[\"input_ids\"] for b in batch]\n",
    "        attention_mask = [b[\"attention_mask\"] for b in batch]\n",
    "        labels = [b[\"labels\"] for b in batch]\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            attention_mask, batch_first=True, padding_value=0\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=-100 \n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fea953-726c-45a6-bf7c-78cf0e9e883b",
   "metadata": {
    "id": "b5fea953-726c-45a6-bf7c-78cf0e9e883b"
   },
   "outputs": [],
   "source": [
    "def generate_neutral_article(left, right, model, tokenizer, max_length=4096, max_new_tokens=2048):\n",
    "    prompt = f\"<|left|>\\n{left}\\n<|right|>\\n{right}\\n<|neutral|>\\n\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"<|neutral|>\" in generated_text:\n",
    "        generated_neutral = generated_text.split(\"<|neutral|>\")[-1].strip()\n",
    "    else:\n",
    "        generated_neutral = generated_text.split(prompt)[-1].strip()\n",
    "\n",
    "    return generated_neutral\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_df, metrics_dict):\n",
    "    model.eval()\n",
    "\n",
    "    generated_list = []\n",
    "    reference_list = []\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
    "        gen = generate_neutral_article(\n",
    "            row[\"left\"],\n",
    "            row[\"right\"],\n",
    "            model,\n",
    "            tokenizer,\n",
    "            max_length=CONFIG[\"max_seq_length\"] - CONFIG[\"max_new_tokens\"],\n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"]\n",
    "        )\n",
    "        ref = row[\"neutral\"]\n",
    "\n",
    "        generated_list.append(gen)\n",
    "        reference_list.append(ref)\n",
    "\n",
    "        if idx % 25 == 0:\n",
    "            print(f\"\\nSample {idx}:\")\n",
    "            print(f\"Generated: {gen}\")\n",
    "            print(f\"Reference: {ref}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    rouge_result = metrics_dict[\"rouge\"].compute(\n",
    "        predictions=generated_list,\n",
    "        references=reference_list,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    tokenized_gen = [nltk.word_tokenize(text.lower()) for text in generated_list]\n",
    "    tokenized_ref = [[nltk.word_tokenize(text.lower())] for text in reference_list]\n",
    "\n",
    "    bleu_result = metrics_dict[\"bleu\"].compute(\n",
    "        predictions=tokenized_gen,\n",
    "        references=tokenized_ref\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(\"ROUGE scores:\", {k: v.mid.fmeasure for k, v in rouge_result.items()})\n",
    "    print(\"BLEU score:\", bleu_result[\"bleu\"])\n",
    "\n",
    "    results = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"].mid.fmeasure,\n",
    "        \"rouge2\": rouge_result[\"rouge2\"].mid.fmeasure,\n",
    "        \"rougeL\": rouge_result[\"rougeL\"].mid.fmeasure,\n",
    "        \"bleu\": bleu_result[\"bleu\"],\n",
    "    }\n",
    "\n",
    "    return results, generated_list, reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5951a33-ab49-4e77-86a7-0c1c8c3fd859",
   "metadata": {
    "id": "e5951a33-ab49-4e77-86a7-0c1c8c3fd859",
    "outputId": "c198024a-db0a-4994-b2b7-04a9487973e6"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_dir = os.path.join(CONFIG[\"root_dir\"], CONFIG[\"output_dir\"])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n===== LOADING DATA =====\")\n",
    "    combined_data = load_all_data(CONFIG[\"json_files\"], CONFIG[\"root_dir\"])\n",
    "    triplet_data = generate_triplets(combined_data)\n",
    "    df_triplets = pd.DataFrame(triplet_data)\n",
    "\n",
    "    print(f\"Dataset size: {len(df_triplets)} examples\")\n",
    "    text_lengths = df_triplets.apply(\n",
    "        lambda x: len(x['left'].split()) + len(x['right'].split()) + len(x['neutral'].split()),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"Average total word count per example: {text_lengths.mean():.1f}\")\n",
    "    print(f\"Max total word count per example: {text_lengths.max()}\")\n",
    "\n",
    "\n",
    "    output_text_lengths = df_triplets.apply(\n",
    "        lambda x: len(x['neutral'].split()),\n",
    "        axis=1\n",
    "    )\n",
    "    estimated_output_token_lengths = output_text_lengths * 1.3\n",
    "    print(f\"Estimated average token count per triplet: {estimated_output_token_lengths.mean():.1f}\")\n",
    "    print(f\"Estimated max token count per triplet: {estimated_output_token_lengths.max():.1f}\")\n",
    "    print(f\"Number of examples that may exceed 6500 tokens: {(estimated_output_token_lengths > 2048).sum()}\")\n",
    "\n",
    "\n",
    "    estimated_token_lengths = text_lengths * 1.3\n",
    "    print(f\"Estimated average token count per triplet: {estimated_token_lengths.mean():.1f}\")\n",
    "    print(f\"Estimated max token count per triplet: {estimated_token_lengths.max():.1f}\")\n",
    "    print(f\"Number of examples that may exceed 6500 tokens: {(estimated_token_lengths > 6500).sum()}\")\n",
    "\n",
    "    token_bins = [0, 2000, 4000, 6000, 8000, float('inf')]\n",
    "    token_counts = pd.cut(estimated_token_lengths, bins=token_bins).value_counts().sort_index()\n",
    "    print(\"Estimated token length distribution:\")\n",
    "    for i, count in enumerate(token_counts):\n",
    "        if i < len(token_bins) - 1:\n",
    "            print(f\"  {token_bins[i]}-{token_bins[i+1]}: {count} examples\")\n",
    "\n",
    "\n",
    "    print(\"\\n===== SPLITTING DATA =====\")\n",
    "    df_train_val, df_test = train_test_split(df_triplets, test_size=0.1, random_state=42)\n",
    "    df_train, df_val = train_test_split(df_train_val, test_size=0.1, random_state=42)\n",
    "    print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
    "\n",
    "    print(\"\\n===== SETTING UP TOKENIZER =====\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    special_tokens_dict = {\n",
    "        \"additional_special_tokens\": [\"<|neutral|>\", \"<|left|>\", \"<|right|>\"],\n",
    "        \"bos_token\": \"<|neutral|>\"\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    print(\"\\n===== SETTING UP MODEL WITH QLORA =====\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # for name, _ in model.named_modules():\n",
    "    #     print(name)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        target_modules=CONFIG[\"target_modules\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    train_dataset = TripletDataset(df_train, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
    "    val_dataset = TripletDataset(df_val, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=CONFIG[\"per_device_batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"per_device_batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=CONFIG[\"evaluate_every\"],\n",
    "        save_strategy=CONFIG[\"save_strategy\"],\n",
    "        save_steps=CONFIG[\"evaluate_every\"],\n",
    "        logging_steps=10,\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "        warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        eval_accumulation_steps=CONFIG[\"eval_accumulation_steps\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== SETTING UP TRAINER =====\")\n",
    "    data_collator = DataCollator(tokenizer)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== STARTING TRAINING =====\")\n",
    "    try:\n",
    "        trainable = any(p.requires_grad for p in model.parameters())\n",
    "        trainer.train()\n",
    "        model.save_pretrained(os.path.join(output_dir, \"adapters\"))\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, \"tokenizer\"))\n",
    "        print(f\"Model saved to {os.path.join(output_dir, 'adapters')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n===== EVALUATING MODEL =====\")\n",
    "    metrics = {\n",
    "        \"rouge\": evaluate.load(\"rouge\"),\n",
    "        \"bleu\": evaluate.load(\"bleu\")\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "    eval_results, generated_samples, reference_samples = evaluate_model(\n",
    "        model, tokenizer, df_test, metrics\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\n===== SAVING RESULTS =====\")\n",
    "    with open(os.path.join(output_dir, \"eval_metrics.json\"), \"w\") as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "\n",
    "    examples = []\n",
    "    for i in range(len(generated_samples)):\n",
    "        examples.append({\n",
    "            \"left\": df_test.iloc[i][\"left\"],\n",
    "            \"right\": df_test.iloc[i][\"right\"],\n",
    "            \"generated\": generated_samples[i],\n",
    "            \"reference\": reference_samples[i]\n",
    "        })\n",
    "\n",
    "    with open(os.path.join(output_dir, \"sample_outputs.json\"), \"w\") as f:\n",
    "        json.dump(examples, f, indent=2)\n",
    "\n",
    "    print(f\"Evaluation results saved to {output_dir}\")\n",
    "    print(\"\\n===== TRAINING COMPLETE =====\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL 100",
   "language": "python",
   "name": "dl_100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
