{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddc49e5-bd93-425a-bc47-179dc6a7ff26",
   "metadata": {
    "id": "2ddc49e5-bd93-425a-bc47-179dc6a7ff26",
    "outputId": "9889a3d7-dba9-4351-c318-21f7d97a81ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInstructions to install packages and libraries:\\n1. !nvidia-smi (Check CUDA)\\nhttps://www.carc.usc.edu/user-guides/data-science/building-conda-environment\\n2. module purge\\n3. module load conda\\n4. mamba init bash\\n5. source ~/.bashrc\\n6. mamba create --name <env_name>\\n7. mamba activate <env_name>\\n8. mamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\\n9. mamba install other libraries\\n10. Run a batch job\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Instructions to install packages and libraries:\n",
    "1. !nvidia-smi (Check CUDA)\n",
    "https://www.carc.usc.edu/user-guides/data-science/building-conda-environment\n",
    "2. module purge\n",
    "3. module load conda\n",
    "4. mamba init bash\n",
    "5. source ~/.bashrc\n",
    "6. mamba create --name <env_name>\n",
    "7. mamba activate <env_name>\n",
    "8. mamba install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "9. mamba install other libraries\n",
    "10. Run a batch job\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bPgZ_AIQFxya",
   "metadata": {
    "id": "bPgZ_AIQFxya"
   },
   "outputs": [],
   "source": [
    "# Batch Job Script. May have to change the env\n",
    "# #!/bin/bash\n",
    "\n",
    "# #SBATCH --account=yzhao010_1531\n",
    "# #SBATCH --partition=gpu\n",
    "# #SBATCH --nodes=1\n",
    "# #SBATCH --ntasks=1\n",
    "# #SBATCH --cpus-per-task=4\n",
    "# #SBATCH --gpus-per-task=a100:1\n",
    "# #SBATCH --mem=16G\n",
    "# #SBATCH --time=2:30:00\n",
    "# #SBATCH --output=/home1/kharwal/logs_stdout.txt\n",
    "# #SBATCH --error=/home1/kharwal/logs_stderr.txt\n",
    "\n",
    "# module purge\n",
    "# module load conda\n",
    "# eval \"$(conda shell.bash hook)\"\n",
    "\n",
    "# mamba activate /home1/kharwal/.conda/envs/dl_100\n",
    "\n",
    "# python dl_100.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a12e0d-8e05-43e2-9f3b-615ec599217c",
   "metadata": {
    "id": "19a12e0d-8e05-43e2-9f3b-615ec599217c",
    "outputId": "6f0e7179-181d-4945-8ddb-66955ecfe035"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/kharwal/.conda/envs/dl_100/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "GPU Memory: 85.10 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Political Neutrality Model: Fine-tuning MPT-7B for neutral article generation\n",
    "-------------------\n",
    "This script fine-tunes a 7B parameter model to generate politically neutral\n",
    "content given left-leaning and right-leaning articles using QLoRA techniques.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# For evaluation\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt_tab', quiet=True)  # Needed for BLEU score calculation\n",
    "\n",
    "# Set environment variables for better performance\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ff50da-5d48-4fb9-8e64-441cd4f6ebc2",
   "metadata": {
    "id": "d1ff50da-5d48-4fb9-8e64-441cd4f6ebc2"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"mosaicml/mpt-7b-8k-instruct\",  # Model with 8K context window\n",
    "    \"root_dir\": \"/home1/kharwal/\",              # Base directory for data and outputs\n",
    "    \"output_dir\": \"mpt-lora-neutral\",           # Output directory name\n",
    "    \"max_seq_length\": 6000,                     # Maximum sequence length for training (increased for triplets)\n",
    "    \"per_device_batch_size\": 1,                 # Per device batch size (reduced for longer sequences)\n",
    "    \"gradient_accumulation_steps\": 8,           # For effective batch size of 8 (adjusted for batch size=1)\n",
    "    \"learning_rate\": 2e-4,                      # Learning rate for LoRA fine-tuning\n",
    "    \"num_train_epochs\": 5,     #TODO                 # Number of training epochs\n",
    "    \"lora_r\": 8,                                # LoRA attention dimension\n",
    "    \"lora_alpha\": 16,                           # LoRA alpha parameter\n",
    "    \"lora_dropout\": 0.05,                       # Dropout probability for LoRA layers\n",
    "    \"weight_decay\": 0.01,                       # Weight decay for AdamW\n",
    "    \"warmup_steps\": 25,                         # Linear warmup steps\n",
    "    \"target_modules\": [\"Wqkv\", \"out_proj\"],     # MPT-specific target modules for LoRA\n",
    "    \"max_new_tokens\": 1800,                     # Max new tokens for generation (increased as requested)\n",
    "    \"evaluate_every\": 30,                      # Evaluate every N steps\n",
    "    \"save_total_limit\": 2,                      # Number of checkpoints to save\n",
    "    \"eval_accumulation_steps\": 8,               # Accumulation steps for evaluation\n",
    "    \"save_strategy\": \"steps\",                   # Save strategy (steps or epoch)\n",
    "    \"json_files\": [                             # Data files\n",
    "        \"flipside_4K_articles_23-25.json\",\n",
    "        \"flipside_4k_articles_21-22.json\",\n",
    "        \"flipside_4k_articles_19-20.json\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f30d55-a720-4bec-b97a-a4e58a9b38f7",
   "metadata": {
    "id": "06f30d55-a720-4bec-b97a-a4e58a9b38f7"
   },
   "outputs": [],
   "source": [
    "# ----------------- DATA LOADING -----------------\n",
    "def load_all_data(file_paths: List[str], root_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load and combine all JSON data files.\"\"\"\n",
    "    all_entries = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(root_dir, path)\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"Loading data from {full_path}\")\n",
    "            with open(full_path, \"r\") as f:\n",
    "                all_entries.extend(json.load(f))\n",
    "        else:\n",
    "            print(f\"[WARN] File not found: {full_path}\")\n",
    "\n",
    "    print(f\"Loaded {len(all_entries)} total entries from {len(file_paths)} files\")\n",
    "    return all_entries\n",
    "\n",
    "def generate_triplets(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate left-right-neutral article triplets from raw data.\"\"\"\n",
    "    triplets = []\n",
    "\n",
    "    for entry in data:\n",
    "        left_articles = entry.get(\"left\", [])\n",
    "        right_articles = entry.get(\"right\", [])\n",
    "        neutral_articles = entry.get(\"neutral\", [])\n",
    "\n",
    "        # Skip if any perspective is missing\n",
    "        if not (left_articles and right_articles and neutral_articles):\n",
    "            continue\n",
    "\n",
    "        # Create cycles to handle uneven article counts\n",
    "        left_cycle = cycle(left_articles)\n",
    "        right_cycle = cycle(right_articles)\n",
    "\n",
    "        for neutral in neutral_articles:\n",
    "            left = next(left_cycle)\n",
    "            right = next(right_cycle)\n",
    "\n",
    "            # Combine headline and text for each article\n",
    "            left_combined = f\"{left['headline']}. {left['text']}\"\n",
    "            right_combined = f\"{right['headline']}. {right['text']}\"\n",
    "            neutral_combined = f\"{neutral['headline']}. {neutral['text']}\"\n",
    "\n",
    "            triplets.append({\n",
    "                \"left\": left_combined,\n",
    "                \"right\": right_combined,\n",
    "                \"neutral\": neutral_combined,\n",
    "                \"meta\": {\n",
    "                    \"event_id\": entry[\"id\"],\n",
    "                    \"date\": entry[\"date\"],\n",
    "                    \"source_title\": entry[\"source_title\"]\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(f\"Generated {len(triplets)} triplets\")\n",
    "    return triplets\n",
    "\n",
    "# ----------------- DATASET CLASS -----------------\n",
    "class TripletDataset(Dataset):\n",
    "    \"\"\"Dataset for processing left-right-neutral article triplets.\"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length=4096):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Format prompt with special tokens\n",
    "        prompt = f\"<|left|>\\n{row['left']}\\n<|right|>\\n{row['right']}\\n<|neutral|>\\n{row['neutral']}\"\n",
    "\n",
    "        # Tokenize with truncation\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone()\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class DataCollator:\n",
    "    \"\"\"Custom data collator for handling variable length sequences.\"\"\"\n",
    "\n",
    "    tokenizer: AutoTokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids = [b[\"input_ids\"] for b in batch]\n",
    "        attention_mask = [b[\"attention_mask\"] for b in batch]\n",
    "        labels = [b[\"labels\"] for b in batch]\n",
    "\n",
    "        # Pad to the longest sequence in the batch\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            attention_mask, batch_first=True, padding_value=0\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=-100  # -100 ignores padding in loss calculation\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fea953-726c-45a6-bf7c-78cf0e9e883b",
   "metadata": {
    "id": "b5fea953-726c-45a6-bf7c-78cf0e9e883b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- EVALUATION FUNCTIONS -----------------\n",
    "def generate_neutral_article(left, right, model, tokenizer, max_length=4096, max_new_tokens=2048):\n",
    "    \"\"\"Generate a neutral article given left and right articles.\"\"\"\n",
    "    # Format prompt with special tokens\n",
    "    prompt = f\"<|left|>\\n{left}\\n<|right|>\\n{right}\\n<|neutral|>\\n\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Use greedy decoding for evaluation\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the generated neutral article (after the last <|neutral|> tag)\n",
    "    if \"<|neutral|>\" in generated_text:\n",
    "        generated_neutral = generated_text.split(\"<|neutral|>\")[-1].strip()\n",
    "    else:\n",
    "        # Fallback if no neutral tag is present\n",
    "        generated_neutral = generated_text.split(prompt)[-1].strip()\n",
    "\n",
    "    return generated_neutral\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_df, metrics_dict):\n",
    "    \"\"\"Evaluate the model using ROUGE and BLEU metrics.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # During inference, we only need to provide left+right articles and generate the neutral article\n",
    "    # The max_new_tokens is set to 2048 to allow for longer generated articles\n",
    "    generated_list = []\n",
    "    reference_list = []\n",
    "\n",
    "    # Generate predictions for each test example\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
    "        # Generate neutral article\n",
    "        gen = generate_neutral_article(\n",
    "            row[\"left\"],\n",
    "            row[\"right\"],\n",
    "            model,\n",
    "            tokenizer,\n",
    "            max_length=CONFIG[\"max_seq_length\"] - CONFIG[\"max_new_tokens\"],\n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"]\n",
    "        )\n",
    "        ref = row[\"neutral\"]\n",
    "\n",
    "        generated_list.append(gen)\n",
    "        reference_list.append(ref)\n",
    "\n",
    "        # Print a sample every 50 examples\n",
    "        if idx % 25 == 0:\n",
    "            print(f\"\\nSample {idx}:\")\n",
    "            print(f\"Generated: {gen}\")\n",
    "            print(f\"Reference: {ref}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_result = metrics_dict[\"rouge\"].compute(\n",
    "        predictions=generated_list,\n",
    "        references=reference_list,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Compute BLEU score\n",
    "    # Tokenize for BLEU (which expects list of tokens)\n",
    "    tokenized_gen = [nltk.word_tokenize(text.lower()) for text in generated_list]\n",
    "    tokenized_ref = [[nltk.word_tokenize(text.lower())] for text in reference_list]\n",
    "\n",
    "    bleu_result = metrics_dict[\"bleu\"].compute(\n",
    "        predictions=tokenized_gen,\n",
    "        references=tokenized_ref\n",
    "    )\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(\"ROUGE scores:\", {k: v.mid.fmeasure for k, v in rouge_result.items()})\n",
    "    print(\"BLEU score:\", bleu_result[\"bleu\"])\n",
    "\n",
    "    # Return metrics for potential logging\n",
    "    results = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"].mid.fmeasure,\n",
    "        \"rouge2\": rouge_result[\"rouge2\"].mid.fmeasure,\n",
    "        \"rougeL\": rouge_result[\"rougeL\"].mid.fmeasure,\n",
    "        \"bleu\": bleu_result[\"bleu\"],\n",
    "    }\n",
    "\n",
    "    return results, generated_list, reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90da5491-8e25-41a6-865f-bd5f4f1597df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        max_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {max_mem:.2f}GB total\")\n",
    "        return allocated < (max_mem * 0.9)  # Return True if we're using less than 90% of memory\n",
    "    return True\n",
    "    \n",
    "def clear_memory():\n",
    "    \"\"\"Aggressive memory clearing function.\"\"\"\n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check memory after clearing\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5951a33-ab49-4e77-86a7-0c1c8c3fd859",
   "metadata": {
    "id": "e5951a33-ab49-4e77-86a7-0c1c8c3fd859",
    "outputId": "c198024a-db0a-4994-b2b7-04a9487973e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LOADING DATA =====\n",
      "Loading data from /home1/kharwal/flipside_4K_articles_23-25.json\n",
      "Loading data from /home1/kharwal/flipside_4k_articles_21-22.json\n",
      "Loading data from /home1/kharwal/flipside_4k_articles_19-20.json\n",
      "Loaded 1037 total entries from 3 files\n",
      "Generated 1376 triplets\n",
      "Dataset size: 1376 examples\n",
      "Average total word count per example: 3105.5\n",
      "Max total word count per example: 20174\n",
      "Estimated average token count per triplet: 1463.3\n",
      "Estimated max token count per triplet: 23159.5\n",
      "Number of examples that may exceed 6500 tokens: 137\n",
      "Estimated average token count per triplet: 4037.2\n",
      "Estimated max token count per triplet: 26226.2\n",
      "Number of examples that may exceed 6500 tokens: 67\n",
      "Estimated token length distribution:\n",
      "  0-2000: 28 examples\n",
      "  2000-4000: 827 examples\n",
      "  4000-6000: 422 examples\n",
      "  6000-8000: 69 examples\n",
      "  8000-inf: 30 examples\n",
      "\n",
      "===== SPLITTING DATA =====\n",
      "Train: 1114, Val: 124, Test: 138\n",
      "\n",
      "===== SETTING UP TOKENIZER =====\n",
      "\n",
      "===== SETTING UP MODEL WITH QLORA =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 6,654,955,520 || trainable%: 0.0945\n",
      "\n",
      "===== SETTING UP TRAINER =====\n",
      "\n",
      "===== STARTING TRAINING =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/695 01:30 < 8:42:09, 0.02 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------- MAIN EXECUTION -----------------\n",
    "def main():\n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(CONFIG[\"root_dir\"], CONFIG[\"output_dir\"])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Load and preprocess data\n",
    "    print(\"\\n===== LOADING DATA =====\")\n",
    "    combined_data = load_all_data(CONFIG[\"json_files\"], CONFIG[\"root_dir\"])\n",
    "    triplet_data = generate_triplets(combined_data)\n",
    "    df_triplets = pd.DataFrame(triplet_data)\n",
    "\n",
    "    # Check data characteristics\n",
    "    print(f\"Dataset size: {len(df_triplets)} examples\")\n",
    "    text_lengths = df_triplets.apply(\n",
    "        lambda x: len(x['left'].split()) + len(x['right'].split()) + len(x['neutral'].split()),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"Average total word count per example: {text_lengths.mean():.1f}\")\n",
    "    print(f\"Max total word count per example: {text_lengths.max()}\")\n",
    "\n",
    "\n",
    "    output_text_lengths = df_triplets.apply(\n",
    "        lambda x: len(x['neutral'].split()),\n",
    "        axis=1\n",
    "    )\n",
    "    estimated_output_token_lengths = output_text_lengths * 1.3\n",
    "    print(f\"Estimated average token count per triplet: {estimated_output_token_lengths.mean():.1f}\")\n",
    "    print(f\"Estimated max token count per triplet: {estimated_output_token_lengths.max():.1f}\")\n",
    "    print(f\"Number of examples that may exceed 6500 tokens: {(estimated_output_token_lengths > 2048).sum()}\")\n",
    "\n",
    "\n",
    "    # Estimate token counts (words * 1.3 is a rough approximation)\n",
    "    estimated_token_lengths = text_lengths * 1.3\n",
    "    print(f\"Estimated average token count per triplet: {estimated_token_lengths.mean():.1f}\")\n",
    "    print(f\"Estimated max token count per triplet: {estimated_token_lengths.max():.1f}\")\n",
    "    print(f\"Number of examples that may exceed 6500 tokens: {(estimated_token_lengths > 6500).sum()}\")\n",
    "\n",
    "    # Display token length distribution\n",
    "    token_bins = [0, 2000, 4000, 6000, 8000, float('inf')]\n",
    "    token_counts = pd.cut(estimated_token_lengths, bins=token_bins).value_counts().sort_index()\n",
    "    print(\"Estimated token length distribution:\")\n",
    "    for i, count in enumerate(token_counts):\n",
    "        if i < len(token_bins) - 1:\n",
    "            print(f\"  {token_bins[i]}-{token_bins[i+1]}: {count} examples\")\n",
    "\n",
    "    # 2. Split data\n",
    "    print(\"\\n===== SPLITTING DATA =====\")\n",
    "    # Stratified split not necessary for this task; use random split\n",
    "    df_train_val, df_test = train_test_split(df_triplets, test_size=0.1, random_state=42)\n",
    "    df_train, df_val = train_test_split(df_train_val, test_size=0.1, random_state=42)\n",
    "    print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
    "\n",
    "    # 3. Set up tokenizer\n",
    "    print(\"\\n===== SETTING UP TOKENIZER =====\")\n",
    "    # Note: We're using a max_seq_length of 6500 tokens during training to accommodate\n",
    "    # all three articles (left + right + neutral) plus special tokens in the input.\n",
    "    # This is important because during fine-tuning we provide the complete triplet,\n",
    "    # while at inference time we only need to provide left + right articles.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"], padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Add special tokens for our task\n",
    "    special_tokens_dict = {\n",
    "        \"additional_special_tokens\": [\"<|neutral|>\", \"<|left|>\", \"<|right|>\"],\n",
    "        \"bos_token\": \"<|neutral|>\"\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # 4. Set up model with QLoRA\n",
    "    print(\"\\n===== SETTING UP MODEL WITH QLORA =====\")\n",
    "\n",
    "    # # Configure quantization\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch.float16\n",
    "    # )\n",
    "\n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        # quantization_config=bnb_config,\n",
    "        # max_position_embeddings=CONFIG[\"max_seq_length\"],  # Ensure model supports our longer sequence length\n",
    "    )\n",
    "\n",
    "    # for name, _ in model.named_modules():\n",
    "    #     print(name)\n",
    "\n",
    "    # Resize token embeddings for new special tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        target_modules=CONFIG[\"target_modules\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "\n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # 5. Create datasets\n",
    "    train_dataset = TripletDataset(df_train, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
    "    val_dataset = TripletDataset(df_val, tokenizer, max_length=CONFIG[\"max_seq_length\"])\n",
    "\n",
    "    # 6. Set up training arguments\n",
    "    # Note: With the increased sequence length (6500), we need to be cautious of memory usage\n",
    "    # This is why we reduced batch size to 1 and increased gradient accumulation steps to 8\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=CONFIG[\"per_device_batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"per_device_batch_size\"],\n",
    "        gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=CONFIG[\"evaluate_every\"],\n",
    "        save_strategy=CONFIG[\"save_strategy\"],\n",
    "        save_steps=CONFIG[\"evaluate_every\"],\n",
    "        logging_steps=10,\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "        warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "        eval_accumulation_steps=CONFIG[\"eval_accumulation_steps\"]\n",
    "    )\n",
    "\n",
    "    # 7. Set up trainer\n",
    "    print(\"\\n===== SETTING UP TRAINER =====\")\n",
    "    data_collator = DataCollator(tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # 8. Train the model\n",
    "    print(\"\\n===== STARTING TRAINING =====\")\n",
    "    try:\n",
    "        trainable = any(p.requires_grad for p in model.parameters())\n",
    "        if not trainable:\n",
    "            raise ValueError(\"No parameters require gradients. LoRA adapter may not be properly applied.\")\n",
    "\n",
    "        trainer.train()\n",
    "        # Save the trained model and tokenizer\n",
    "        model.save_pretrained(os.path.join(output_dir, \"adapters\"))\n",
    "        tokenizer.save_pretrained(os.path.join(output_dir, \"tokenizer\"))\n",
    "        print(f\"Model saved to {os.path.join(output_dir, 'adapters')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # # 9. Evaluate the model\n",
    "    # print(\"\\n===== EVALUATING MODEL =====\")\n",
    "    # # Load metrics\n",
    "    # metrics = {\n",
    "    #     \"rouge\": evaluate.load(\"rouge\"),\n",
    "    #     \"bleu\": evaluate.load(\"bleu\")\n",
    "    # }\n",
    "\n",
    "    # # Make sure model is in evaluation mode\n",
    "    # model.eval()\n",
    "\n",
    "    # # Run evaluation\n",
    "    # eval_results, generated_samples, reference_samples = evaluate_model(\n",
    "    #     model, tokenizer, df_test, metrics\n",
    "    # )\n",
    "\n",
    "    # # 10. Save evaluation results\n",
    "    # print(\"\\n===== SAVING RESULTS =====\")\n",
    "    # # Save metrics\n",
    "    # with open(os.path.join(output_dir, \"eval_metrics.json\"), \"w\") as f:\n",
    "    #     json.dump(eval_results, f, indent=2)\n",
    "\n",
    "    # # Save a few examples\n",
    "    # examples = []\n",
    "    # for i in range(len(generated_samples)):\n",
    "    #     examples.append({\n",
    "    #         \"left\": df_test.iloc[i][\"left\"],  # Truncate for readability\n",
    "    #         \"right\": df_test.iloc[i][\"right\"],\n",
    "    #         \"generated\": generated_samples[i],\n",
    "    #         \"reference\": reference_samples[i]\n",
    "    #     })\n",
    "\n",
    "    # with open(os.path.join(output_dir, \"sample_outputs.json\"), \"w\") as f:\n",
    "    #     json.dump(examples, f, indent=2)\n",
    "\n",
    "    # print(f\"Evaluation results saved to {output_dir}\")\n",
    "    print(\"\\n===== TRAINING COMPLETE =====\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear CUDA cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434d59e-e511-43f7-b688-e4f79e96de46",
   "metadata": {
    "id": "f434d59e-e511-43f7-b688-e4f79e96de46"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fd0bd-44ac-4abd-b242-5e39b3d849d9",
   "metadata": {
    "id": "962fd0bd-44ac-4abd-b242-5e39b3d849d9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL 100",
   "language": "python",
   "name": "dl_100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
